{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqjfGZoPyuLS"
   },
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_BGRPx5yuLT"
   },
   "source": [
    "> **Embedding mathematically represents a mapping , f: X-> Y, which is a function**. \n",
    "\n",
    "\n",
    "Where the function is\n",
    "\n",
    "•\t**injective** (which is what we call an **injective function** , each Y has a unique X correspondence, and vice versa)\n",
    "\n",
    "•\t**structure-preserving** ( structure preservation , for example, X1 < X2 in the space to which X belongs, then the same applies to Y1 <Y2 in the space to which Y belongs after mapping).\n",
    "\n",
    "> So for word embedding, the word word is mapped to another space, where this mapping has the characteristics of injective and structure-preserving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjoWsQGFyuLU"
   },
   "source": [
    "Popular translation can be considered as word embedding, **which is to map the words in the space to which X belongs to a multi-dimensional vector in Y space , then the multi-dimensional vector is equivalent to embedding in the space to which Y belongs** , one carrot and one pit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_ZB0ZhOyuLV"
   },
   "source": [
    "1. Text data needs to be pre-processed into tensor form before it can be input to the neural network.\n",
    "2. The process of dividing text into units is called tokenization, and the unit of division is called tokens.\n",
    "3. Text can be divided into words, characters (abcdefg ...), n-gram and so on.\n",
    "4. Generally use one-hot encoding or word-embedding to process words into numerical tensors.\n",
    "5. One-hot encoding is simple, but without structure, the distance between any two words is √2.\n",
    "6. The word-embedding space has small dimensions, structure in space, similar words are near, and unrelated words are far away.\n",
    "7. The role of the embedding layer can actually be seen as a matrix that maps the points in the high-dimensional space to the low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dR0h4UOWyuLW"
   },
   "source": [
    "Word embedding is a form of word representation **that connects the human understanding of language to that of the machine**. Word embeddings are the distributed representations of text in an ample dimensional space. By looking at different researches in the area of deep learning, word embeddings are essential. **It is the approach of representing words and documents that may be considered as one of the crucial breakthroughs in the field of deep learning on challenging NLP problems.**\n",
    "\n",
    "Word embeddings are a class of techniques **where the individual word, is represented as a real-valued vector in a vector space**. The main idea is to use a densely distributed representation for all the words.\n",
    "Each word is represented by a real-value vector. Each word is mapped to a single vector, and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n",
    "\n",
    "This is contrasted to the thousands of dimensions required for sparse word representations, such as **One-Hot Encoding**. **They are essential for solving most NLP problems**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70z-3lMhyuLW"
   },
   "source": [
    "**The neural network cannot train the original text data. We need to process the text data into numerical tensors first. This process is also called text vectorization.**\n",
    "\n",
    "There are several strategies for text vectorization:\n",
    "1. Split text into words, each word is converted into a vector\n",
    "2. Split text into characters, each character is converted into a vector\n",
    "3. Extract n-gram of words or characters n-gram to a vector\n",
    "\n",
    "**The unit into which text is decomposed is called token, and the process of decomposing text into token is called tokenization.**\n",
    "\n",
    "To put it simply, we need to input text data into a neural network and let it train. However, neural networks cannot directly process text data. We need to pre-process text data into a format that the neural network can understand, which is the following process:\n",
    "\n",
    "**Text ----> Participle ----> Vectorization**\n",
    "\n",
    "\n",
    "There are two main methods for word vectorization:\n",
    "1. One-hot encoding\n",
    "2. Word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W9QEQiVQyuLX"
   },
   "source": [
    "# **One-Hot encoding**\n",
    "\n",
    "**Why is it called one-hot?** \n",
    "\n",
    "**After each word is one-hot encoded, only one position has an element of 1 and the other positions are all 0.**\n",
    "\n",
    "For example, \n",
    "the sentence **\"the boy is crying\"** (assuming there are only four English words in the world), after one-hot encoding,\n",
    "\n",
    "**the corresponds to (1, 0, 0, 0)**\n",
    "\n",
    "**boy corresponds to (0, 1, 0 ， 0）**\n",
    "\n",
    "**is corresponds to (0,0,1,0)**\n",
    "\n",
    "**crying corresponds to (0,0,0,1)**\n",
    "\n",
    "Each word corresponds to a position in the vector, and this position represents the word.\n",
    "\n",
    "But this way requires a very high dimension, because if all vocabularies have 100,000 words, then each word needs to be represented by a vector of length 100,000.\n",
    "\n",
    "**the corresponding to (1, 0, 0, 0, ..., 0) (length is 100,000)**\n",
    "\n",
    "**boy corresponding to (0, 1, 0, 0, ..., 0)**\n",
    "\n",
    "**is corresponding to (0, 0, 1, 0 , ..., 0)**\n",
    "\n",
    "**crying corresponds to (0,0,0,1, ..., 0) to get high-dimensional sparse tensors.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ctwBKW9yuLY"
   },
   "source": [
    "![img_credit_tensorflow.org](img/one.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4K2mc5nyuLY"
   },
   "source": [
    "### Disadvatages of One HotEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBQJFYbByuLZ"
   },
   "source": [
    "One-Hot coding is simple and easy to use, the disadvantages are also obvious:\n",
    "\n",
    ">The length of the word vector is equal to the length of the vocabulary, and the word vector is extremely sparse. When the vocabulary is large, the computational complexity will be very large.\n",
    "\n",
    ">Any two words are orthogonal, meaning that the relationship between words cannot be obtained from the One-Hot code\n",
    "\n",
    ">The distance between any two words is equal, and the semantic relevance of the two words cannot be reflected from the distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hiSfIqPyuLa"
   },
   "source": [
    "## **Embedding**\n",
    "In contrast, word embedding embeds words into a low-dimensional dense space.\n",
    "\n",
    "For example, the same **\"the boy is crying\"** sentence (assuming that there are only 4 English words in the world), after encoding, it may become:\n",
    "\n",
    "**the corresponding (0.1)**\n",
    "\n",
    "**boy corresponding (0.14)**\n",
    "\n",
    "**is corresponding (0)**\n",
    "\n",
    "**crying corresponding (0.82)**\n",
    "\n",
    "We assume that the embedded space is 256 dimensions (generally 256, 512 or 1024 dimensions, the larger the vocabulary, the higher the corresponding spatial dimension)\n",
    "\n",
    "**Then\n",
    "the corresponding (\n",
    "0.1,\n",
    "0.2, 0.4,\n",
    "0 , ...) (vector length is 256) boy corresponds to (0.23, 0.14, 0, 0 , ...) is corresponding to (0, 0 , 0.41, 0.9, ...) , 0.82, 0, 0.14, ...)**\n",
    "\n",
    "One-hot encoding is very simple, but the spatial dimension is high and for one-hot encoding, the distance between any two words is $$\\sqrt{2}$$.\n",
    "\n",
    " But in practice, the word **(boy) to word (man) should be very close** (because they are closely related), and **the word (cat) to word (stone) should be very far** (because they are basically unrelated).\n",
    "\n",
    "Embedding space has low dimensions and allows space to have structure .\n",
    "\n",
    "For example, the distance between the vectors can reflect gender, age, etc. (this requires training, and the unembedding layer has no structure), for example:\n",
    "\n",
    "**man-woman = boy-girl**\n",
    "\n",
    "**man-daddy = woman-mother**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5bfuJv-9yuLa"
   },
   "source": [
    "In Keras, the Embedding layer requires two parameters, one is the number of words in the token, and the other is the embedded dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IF5F9jyCyuLb",
    "outputId": "d6141b32-5799-426e-ea3c-43972b54c664"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2SEy1dnWyuLh"
   },
   "source": [
    "1000 : The length of the token is 1000 (can be considered as the number of all words in the vocabulary)\n",
    "\n",
    "64 : Represents embedded 64-dimensional space (64 attributes can be considered, such as imaginary adult eye shape, nose shape, mouth shape, height, weight, age, etc., together, it is a person (word). A word, a thousand such words means all the words in the vocabulary)\n",
    "\n",
    "Embedding layer input : a two-dimensional tensor with the shape (samples, sequential_length)\n",
    "samples: represent different sentences.\n",
    "sequential_length: represents the number of words in the sentence, each word corresponds to a number, a total of sequential_length words.\n",
    "\n",
    "The output of the embedding layer : a three-dimensional tensor with the shape (samples, sequential_length, dimensionality)\n",
    "samples: Represent different sentences.\n",
    "sequential_length: represents the number of words in a sentence.\n",
    "dimensionality: represents the number of channels. A vector of values ​​on all channels on the same samples and the same sequential_length represents a word, such as (0,0, :) represents a word.\n",
    "\n",
    "The embedding layer can be regarded as a matrix , assuming that the input is (100, 20), 100 sequences of length 20, the vocabulary length is 10000, Embedding (10000, 8), and the output is (100, 20, 8)\n",
    "because After one-hot encoding, each sequence can be regarded as (20, 10000), and the matrix of (10000, 8) is multiplied to get the matrix of (20, 8), so 100 such sequences pass through the embedding layer. Becomes (100, 20, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptRi7XJiyuLi",
    "outputId": "34b58bfc-d721-4d92-a4c6-4123c800a669"
   },
   "outputs": [],
   "source": [
    "#Instantiate an Embedding layer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000,8,input_length=20))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dH7eqsf_yuLl",
    "outputId": "227a3c50-a8d1-41a0-8547-c885ab94f2e0"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Dense,Embedding\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=10000)\n",
    "#(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=maxlen)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(10000,8,input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1,activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--uzhSSayuLp",
    "outputId": "65e4b9bb-9176-4957-ffd6-00d64c052989"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uoHp5o9TyuLt"
   },
   "source": [
    "#### It can be seen that in the embedding layer, we need to train 8 times 10000 = 80,000 parameters. Each row in the trained embedding layer represents a vector of words.\n",
    "\n",
    ">Use the Embedding layer and classifier on IMDB data.\n",
    "\n",
    ">The imdb data set built in keras has classified positive and negative evaluations and vectorized evaluation content. We use the Embedding layer and classifier to train a neural network. How well did it perform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vk7-H5FpyuLu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JqyQ54_kyuL0"
   },
   "source": [
    "#### Draw a picture and observe a wave carefully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lk_C-IpVyuL1",
    "outputId": "c8ca8d99-e1c0-4a6e-a284-fb3cf0d58502"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc=history.history['acc']\n",
    "val_acc=history.history['val_acc']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(1,len(acc)+1)\n",
    "\n",
    "plt.plot(epochs,acc,'bo',label='acc')\n",
    "plt.plot(epochs,val_acc,'b',label='val_acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs,loss,'bo',label='loss')\n",
    "plt.plot(epochs,val_loss,'b',label='val_loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_9w0BPMyuL4"
   },
   "source": [
    "#### It can be seen that the verification accuracy is about 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lf3CNxioyuL4"
   },
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P53EmPynyuL5"
   },
   "source": [
    "**Bag-of-words model is a commonly used document representation method in the field of information retrieval .**\n",
    "\n",
    ">In information retrieval, the BOW model assumes that for a document, it ignores its word order, grammar, syntax and other factors, and treats it as a collection of several words. The appearance of each word in the document is independent and independent of whether other words appear. **(It's out of order)**\n",
    "\n",
    ">The Bag-of-words model (BoW model) ignores the grammar and word order of a text, and uses a set of unordered words to express a text or a document.\n",
    "\n",
    "#### Let's take an example\n",
    "\n",
    "`John likes to watch movies. Mary likes too.`\n",
    "\n",
    "`John also likes to watch football games.`\n",
    "\n",
    "Build a dictionary based on the words that appear in the above two sentences:\n",
    "\n",
    "`{\"John\": 1, \"likes\": 2, \"to\": 3, \"watch\": 4, \"movies\": 5, \"also\": 6, \"football\": 7, \"games\": 8, \"Mary\": 9, \"too\": 10}`\n",
    "\n",
    "\n",
    "The dictionary contains 10 words, each word has a unique index. Note that their order is not related to the order in which they appear in the sentence. According to this dictionary, we re-express the above two sentences into the following two vectors:\n",
    "\n",
    "`[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]`\n",
    "\n",
    "`[1, 1, 1, 1, 0, 1, 1, 1, 0, 0]`\n",
    "\n",
    "\n",
    "These two vectors contain a total of 10 elements, where the i-th element represents the number of times the i-th word in the dictionary appears in the sentence. \n",
    "\n",
    "Now imagine a **huge document set D with a total of M documents**. After all the words in the document are extracted, they form a dictionary containing N words. Using the Bag-of-words model, **each document can be represented as an N-dimensional vector**.\n",
    "\n",
    "\n",
    "Therefore, the BoW model can be considered as a statistical histogram. It is used in text retrieval and processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bU0XAU4qyuL5"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zk0Eg4qdyuL6"
   },
   "source": [
    "**TF-IDF (Term Frequency-Inverse Document Frequency)**, a commonly used weighting technique for information retrieval and information exploration.\n",
    "\n",
    "TF-IDF is a statistical method used to evaluate the importance of a word to a file set or a file in a corpus. The importance of the word increases in proportion to the number of times it appears in the file, but at the same time decreases inversely with the frequency of its appearance in the corpus.\n",
    "\n",
    "* **Term frequency TF (item frequency)**: number of times a given word appears in the text. This number is usually normalized (the numerator is generally smaller than the denominator) to prevent it from favoring long documents, because whether the term is important or not, it is likely to appear more often in long documents than in paragraph documents.\n",
    "\n",
    "> **TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).**\n",
    "\n",
    "Term frequency (TF) indicates how often a term (keyword) appears in the text .\n",
    "\n",
    "This number is usually normalized (usually the word frequency divided by the total number of words in the article) to prevent it from favoring long documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sIJIuSpMyuL7"
   },
   "source": [
    "**Formula of Tf**    ![title](img/tf.png)\n",
    "\n",
    "\n",
    "where  ni, j  is the number of occurrences of the word in the file  dj  , and the denominator is the sum of the occurrences of all words in the file dj;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVP7FEGoyuL7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJzZpE-JyuL8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Dwu6mPKyuL8"
   },
   "source": [
    "* **Inverse document frequency (IDF)**: A measure of the general importance of a word. The main idea is that if there are fewer documents containing the entry t and the larger, it means that the entry has a good ability to distinguish categories. The IDF of a specific word can be calculated by dividing the total number of files by the number of files containing the word, and then taking the log of the obtained quotient.\n",
    "\n",
    ">**IDF(t) = log_e(Total number of documents / Number of documents with term t in it).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T26bTkv0yuL9"
   },
   "source": [
    "**Formula of Idf**  ![title](img/idf1.png)\n",
    "\n",
    "\n",
    "among them\n",
    "\n",
    "* | D |: Total number of files in the corpus\n",
    "\n",
    "* |  {  $j: $t_{i}$ \\in $d_{j} $$  }  | : The number of files containing words $t_{i}$ ( $n_{i,j}$ $\\neq$ 0 , the number of files). If the word is not in the corpus, it will cause the dividend to be zero, so it is generally used.1 + |  {  $j :$t_{i}$ \\in $d_{j}$$  }  |.\n",
    "\n",
    "\n",
    "**So, Formula of tf-Idf** ![title](img/ttttt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0n2YUwi1yuL-"
   },
   "source": [
    "#### Example:\n",
    "\n",
    "Consider a document containing 100 words where in the word cat appears 3 times. \n",
    "\n",
    "The **term frequency (Tf) for cat** is then **(3 / 100) = 0.03**. Now, assume we have 10 million documents and the word cat appears in one thousand of these.\n",
    "\n",
    "Then, the **inverse document frequency (Idf)** is calculated as **log(10,000,000 / 1,000) = 4.** \n",
    "\n",
    "Thus, the **Tf-idf** weight is the product of these quantities: **0.03 * 4 = 0.12.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGd8-mdVyuMA"
   },
   "source": [
    "#### TF-IDF application\n",
    "\n",
    "1.  **Search engine**\n",
    "2.  **Keyword extraction**\n",
    "3.  **Text similarity**\n",
    "4.  **Text summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AScJZdWYyuMB"
   },
   "source": [
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3sOgkIoyuMB"
   },
   "source": [
    "### Python3 implements TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stTIGcpayuMC",
    "outputId": "2069fa61-16dd-4bc2-d92e-93cb6f869c65"
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import operator\n",
    " \n",
    "\n",
    "def loadDataSet():\n",
    "    dataset = [ ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],    \n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'] ]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  \n",
    "    return dataset, classVec\n",
    " \n",
    "\n",
    "def feature_select(list_words):\n",
    "    \n",
    "    doc_frequency=defaultdict(int)\n",
    "    for word_list in list_words:\n",
    "        for i in word_list:\n",
    "            doc_frequency[i]+=1\n",
    " \n",
    "    \n",
    "    word_tf={}  \n",
    "    for i in doc_frequency:\n",
    "        word_tf[i]=doc_frequency[i]/sum(doc_frequency.values())\n",
    " \n",
    "    \n",
    "    doc_num=len(list_words)\n",
    "    word_idf={} \n",
    "    word_doc=defaultdict(int) \n",
    "    for i in doc_frequency:\n",
    "        for j in list_words:\n",
    "            if i in j:\n",
    "                word_doc[i]+=1\n",
    "    for i in doc_frequency:\n",
    "        word_idf[i]=math.log(doc_num/(word_doc[i]+1))\n",
    " \n",
    "    \n",
    "    word_tf_idf={}\n",
    "    for i in doc_frequency:\n",
    "        word_tf_idf[i]=word_tf[i]*word_idf[i]\n",
    " \n",
    "    \n",
    "    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return dict_feature_select\n",
    " \n",
    "if __name__=='__main__':\n",
    "    data_list,label_list=loadDataSet() \n",
    "    features=feature_select(data_list) \n",
    "    print(features)\n",
    "    print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WB-5ItW_yuMF",
    "outputId": "42909384-1ae5-406a-a2fc-9fd6dd8c92c6"
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXNYTNgYyuMI"
   },
   "source": [
    "### NLTK implements the TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCjMMcmWyuMI",
    "outputId": "827eb657-f83a-4628-83c6-d7f022bec034"
   },
   "outputs": [],
   "source": [
    "from nltk.text import TextCollection\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "sents=['this is sentence one','this is sentence two','this is sentence three']\n",
    "sents=[word_tokenize(sent) for sent in sents]\n",
    "print(sents)\n",
    "corpus=TextCollection(sents)\n",
    "print(corpus)\n",
    " \n",
    "tf=corpus.tf('one',corpus)\n",
    "print(tf)\n",
    " \n",
    "idf=corpus.idf('one')\n",
    "print(idf)\n",
    " \n",
    "tf_idf=corpus.tf_idf('one',corpus)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w9n624C3yuML"
   },
   "source": [
    "### Sklearn implements TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GfNfRBSyuMM",
    "outputId": "92169ae9-418e-45d1-b05a-99cc1266b71b"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "x_train = ['The main idea of TF-IDF is that algorithm is an important feature that can be separated from the corpus background']\n",
    "x_test=['Original text marked ',' main idea']\n",
    " \n",
    "vectorizer = CountVectorizer(max_features=10)\n",
    "\n",
    "tf_idf_transformer = TfidfTransformer()\n",
    "\n",
    "tf_idf = tf_idf_transformer.fit_transform(vectorizer.fit_transform(x_train))\n",
    "\n",
    "x_train_weight = tf_idf.toarray()\n",
    " \n",
    "\n",
    "tf_idf = tf_idf_transformer.transform(vectorizer.transform(x_test))\n",
    "x_test_weight = tf_idf.toarray()\n",
    " \n",
    "print('Output x_train text vector：')\n",
    "print(x_train_weight)\n",
    "print('Output x_test text vector：')\n",
    "print(x_test_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQe105WeyuMO"
   },
   "outputs": [],
   "source": [
    "Keywords are words that can express the content of the center of a document.\n",
    "Information retrieval and system collection for reader review. Keyword extraction is a branch of the field of text mining.\n",
    "Basic work of text mining research such as document comparison, abstract generation, document classification and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOCWnV-xyuMT"
   },
   "source": [
    "### Jieba implements TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tfac_ExTyuMU",
    "outputId": "2c8bed82-219f-4bd2-ce4e-b0341e66b998"
   },
   "outputs": [],
   "source": [
    "\n",
    "import jieba.analyse\n",
    " \n",
    "text='Keywords are words that can express the content of the center of a document.Information retrieval and system collection for reader review. Keyword extraction is a branch of the field of text mining.Basic work of text mining research such as document comparison, abstract generation, document classification and clustering'\n",
    " \n",
    "keywords=jieba.analyse.extract_tags(text, topK=5, withWeight=False, allowPOS=())\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnibpAlbyuMX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39x0oBreyuMa"
   },
   "source": [
    "# n GRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIKOVLUfyuMa"
   },
   "source": [
    "**Wikipedia definition**: \n",
    "\n",
    "**In computational linguistics, n-gram refers to n consecutive items in the text (items can be phoneme, syllable, letter, word or base pairs)**\n",
    "\n",
    "N-grams of texts are widely used in the field of text mining and natural language processing. They are basically a set of co-occurring words within a defined window and when computing the n-grams, we typically move one word forward or more depending upon the scenario.\n",
    "\n",
    ">For example, for the sentence **“The cow jumps over the moon”**. If **N=2** (known as bigrams), then the ngrams would be:\n",
    "\n",
    "* the cow\n",
    "* cow jumps\n",
    "* jumps over\n",
    "* over the\n",
    "* the moon\n",
    "\n",
    "In n-gram, **n = 1 is unigram**, **n = 2 is bigram**, **n = 3 is trigram**. \n",
    "\n",
    "After **n> 4**, refer directly to numbers, such as **4-gram, 5-gram**.\n",
    "\n",
    "gram is often used to compare sentence similarity, fuzzy query, sentence rationality, sentence correction, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Xs2QJmjyuMa"
   },
   "source": [
    "The n-gram can represent the semantic association reflected by the positional relationship between words. Before explaining the n-gram, we derive from the initial sentence probability.\n",
    "\n",
    "Suppose a sentence S is an ordered arrangement of n words, and is written as: ![title](img/gif.gif)\n",
    "\n",
    "\n",
    "We will abbreviate it as W_ {1} ^ {n}, then the probability of this sentence is: ![title](img/ng.gif)\n",
    "\n",
    "For a single probability, which means the probability that the word appears in the case given by the previous word, we can use Bayesian formula to get: ![title](img/ng1.gif)\n",
    "\n",
    "The last item is the frequency in the corpus. However, long sentences or text after depunctuation may be very long, and the words that are too early have a small impact on the prediction of the word, so we use Markov's hypothesis that the probability of taking the word depends only on the front of the word the n-1 words, this is the idea n-gram model.\n",
    "\n",
    "So the above formula becomes: ![title](img/ng2.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdPjrowYyuMb"
   },
   "source": [
    "#### Determination of N in N-gram\n",
    "\n",
    "To confirm the value of N. \"Language Modeling with Ngrams\" uses the indicator **Perplexity**. The smaller the indicator, the better the effect of a language model. \n",
    "\n",
    ">The article uses a Wall Street Journal database with a dictionary size of 19,979. The training set contains 38 million words and the test set contains 1.5 million words. \n",
    "\n",
    "For different N-grams, calculate their respective purplexity.\n",
    "\n",
    "![title](img/formula.png)\n",
    "\n",
    "The results show that Tri-gram's Perplexity is the smallest, so it works best.\n",
    "![title](img/result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8dpWgzryuMb"
   },
   "source": [
    "### Unigram Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YLq3kK2yuMc",
    "outputId": "ecfa1476-a33b-415f-94ca-a965f01e71e1"
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    " \n",
    "text = \"I am going to the United States\"\n",
    "cut = jieba.cut(text)\n",
    "sent = list(cut)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K-ye2OG2yuMe"
   },
   "source": [
    "### Bigram Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pktQcu1tyuMf",
    "outputId": "7f7072f7-d0a2-4ed3-c608-9adcc9d7974c"
   },
   "outputs": [],
   "source": [
    "Sent = \"I will go to United States\"\n",
    "lst_sent = Sent.split (\" \")\n",
    "of_bigrams_in = []\n",
    "for i in range(len(lst_sent)- 1):\n",
    "   of_bigrams_in.append(lst_sent[i]+ \" \" + lst_sent[ i + 1])\n",
    "   \n",
    "    \n",
    "print(of_bigrams_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cFklgTn9yuMi"
   },
   "source": [
    "### Trigram Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fo-_ANz0yuMj"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "punctuation_pattern = re.compile(r\"\" \"[.,!? \"\"] \"\" \" )\n",
    "\n",
    "sent = \"I will go to United States\"\n",
    "no_punctuation_sent = re.sub(punctuation_pattern , \" \" , sent )\n",
    "lst_sent = no_punctuation_sent.split (\" \")\n",
    "trigram = []\n",
    "for i in range(len(lst_sent)- 2):\n",
    "   trigram.append(lst_sent[i] + \" \" + lst_sent[i + 1] + \" \" +lst_sent[i + 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKrjCaxTyuMl",
    "outputId": "3d7ac081-a5e8-42b6-a92d-8d487c6a0128"
   },
   "outputs": [],
   "source": [
    "trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pvpDZKy1yuMq"
   },
   "source": [
    "## Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hu6-40ZJyuMq"
   },
   "source": [
    ">The co-occurrence matrix is ​​also expressed by considering the relationship between words in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EISAu7YcyuMr"
   },
   "source": [
    ">A very important idea is that we think that the meaning of a word is closely related to the word next to it. This is where we can set a window (the size is generally 5 ~ 10). The size of the window below is 2, so in this window, the words that appear with rests are life, he, in, and peace. Then we use this co-occurrence relationship to generate word vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvezPK9PyuMs"
   },
   "source": [
    "![title](img/concurrence.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0YarWzEPyuMs"
   },
   "source": [
    "For example, our corpus now includes the following three documents:\n",
    "\n",
    "#### I like deep learning.\n",
    "\n",
    "#### I like NLP.\n",
    "\n",
    "#### I enjoy flying.\n",
    "\n",
    "As an example, **we set the window size to 1**, which means that **we only look at the word immediately surrounding a word**. At this point, you will get a symmetric matrix-co-occurrence matrix. Because in our corpus, **the number of times I and like appear as neighbors in the window at the same time is 2**, the value where I and like intersect in the table below is 2. \n",
    "\n",
    "In this way, the idea of turning words into vectors is done. Each row (or each column) of the co-occurrence matrix is a vector representation of the corresponding word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8RAM0SjyuMs"
   },
   "source": [
    "![title](img/concur.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2ke8lX0yuMt"
   },
   "source": [
    ">Although the Cocurrence matrix solves the relative position between words to some extent, this problem should be paid attention to. But it still faces dimensional disaster. \n",
    "\n",
    ">In other words, the vector representation of a word is too long. At this time, it is natural to think of some common dimensionality reduction methods such as SVD or PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DReSxvyOyuMt"
   },
   "source": [
    ">The selection of the window size is the same as determining n in the n-gram. The size of the matrix will also increase when the window is enlarged, so it still has a large amount of calculation in nature, and the SVD algorithm has a large amount of calculation. If the text set is very More, it is not operable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Rtzkmq9yuMu"
   },
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Mw2xbiuyuMu"
   },
   "source": [
    "**GloVe is an unsupervised learning algorithm for obtaining vocabulary vector representations. The aggregated global word co-occurrence statistics from the corpus are trained and the resulting representations show interesting linear substructures of the word vector space.**\n",
    "\n",
    "\n",
    "Official website homepage address: <a href=\"https://nlp.stanford.edu/projects/glove/\" target=\"_blank\">https://nlp.stanford.edu/projects/glove/</a>\n",
    "\n",
    "Github: <a href=\"https://github.com/stanfordnlp/GloVe\" target=\"_blank\">https://github.com/stanfordnlp/GloVe</a>\n",
    "\n",
    "Paper download address: <a href=\"https://nlp.stanford.edu/pubs/glove.pdf\" target=\"_blank\">https://nlp.stanford.edu/pubs/glove.pdf</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbAVxWGHyuMx"
   },
   "source": [
    "#### GloVe word vector format\n",
    "\n",
    "GloVe is a type of Word embedding. The format of the GloVe word vector and word2vec is a little different from the Stanford open source code training. **The first line of the model trained by word2vec is: thesaurus size and dimensions, while gloVe does not**\n",
    "\n",
    "Word2vec training format:\n",
    "\n",
    "    Size Dimension\n",
    "\n",
    "    Word1 vector1\n",
    "    Word2 vector1\n",
    "    ....\n",
    "    WordN vectorN\n",
    "    \n",
    "\n",
    "GloVe training format:\n",
    "\n",
    "\n",
    "    Word1 vector1\n",
    "    Word2 vector1\n",
    "    ....\n",
    "    WordN vectorN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Th6C8UCsyuMy"
   },
   "source": [
    ">Therefore, we use the model trained by Glove to add a line of Vocabulary Size in front, and the model is used in the same way as word2vec. The official website provides a lot of word vector models trained using thesaurus, which can be downloaded and used directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ru8DqzT0yuMz"
   },
   "source": [
    "![title](img/glove.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word Embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
